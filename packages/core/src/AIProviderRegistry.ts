/**
 * AI Provider Registry - Unified AI system across rule engine and content creation
 * Centralizes all AI functionality with provider abstraction and intelligent routing
 */

import { EventEmitter, SystemEvents } from './EventEmitter';
import {
  AIProvider,
  AICapability,
  AITextOptions,
  AITextResponse,
  AIImageOptions,
  AIImageResponse,
  AIAudioOptions,
  AIAudioResponse,
  AICodeOptions,
  AICodeResponse,
  Disposable
} from './SharedInterfaces';

export interface AIProviderConfig {
  apiKey: string;
  baseUrl?: string;
  timeout: number;
  maxRetries: number;
  rateLimitPerMinute: number;
  priority: number;
  enabled: boolean;
}

export interface AIUsageStats {
  totalRequests: number;
  successfulRequests: number;
  failedRequests: number;
  totalTokensUsed: number;
  averageResponseTime: number;
  costEstimate: number;
}

export interface AIRequest {
  id: string;
  type: 'text' | 'image' | 'audio' | 'code' | 'embeddings';
  prompt: string;
  options: any;
  timestamp: Date;
  providerId: string;
  userId?: string;
}

export interface AIResponse {
  requestId: string;
  success: boolean;
  data?: any;
  error?: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  processingTime: number;
  providerId: string;
}

export class OpenAIProvider implements AIProvider {
  public readonly name = 'openai';
  public readonly type = 'chat' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation', 'embeddings'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    const requestBody = {
      model: options.model || 'gpt-4',
      messages: [
        ...(options.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
        ...(options.context || []).map(ctx => ({ role: 'user', content: ctx })),
        { role: 'user', content: prompt }
      ],
      max_tokens: options.maxTokens || 2000,
      temperature: options.temperature || 0.7
    };

    const response = await this.makeRequest('/chat/completions', requestBody);
    
    return {
      text: response.choices[0].message.content,
      usage: {
        promptTokens: response.usage.prompt_tokens,
        completionTokens: response.usage.completion_tokens,
        totalTokens: response.usage.total_tokens
      },
      model: response.model,
      finishReason: response.choices[0].finish_reason === 'stop' ? 'stop' : 'length'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}
    
Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated by AI'
    };
  }

  async createEmbeddings(text: string): Promise<number[]> {
    const response = await this.makeRequest('/embeddings', {
      model: 'text-embedding-ada-002',
      input: text
    });

    return response.data[0].embedding;
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.config.baseUrl || 'https://api.openai.com/v1'}${endpoint}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`OpenAI API error: ${response.statusText}`);
      }

      return response.json();
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class OpenRouterProvider implements AIProvider {
  public readonly name = 'openrouter';
  public readonly type = 'chat' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    const requestBody = {
      model: options.model || 'openrouter/auto',
      messages: [
        ...(options.systemPrompt ? [{ role: 'system', content: options.systemPrompt }] : []),
        ...(options.context || []).map(ctx => ({ role: 'user', content: ctx })),
        { role: 'user', content: prompt }
      ],
      max_tokens: options.maxTokens || 2000,
      temperature: options.temperature || 0.7
    };

    const response = await this.makeRequest('/chat/completions', requestBody);
    
    return {
      text: response.choices[0].message.content,
      usage: {
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0
      },
      model: response.model || requestBody.model,
      finishReason: response.choices[0].finish_reason === 'stop' ? 'stop' : 'length'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}
    
Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3,
      model: 'anthropic/claude-3-sonnet'
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated via OpenRouter'
    };
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.config.baseUrl || 'https://openrouter.ai/api/v1'}${endpoint}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
          'HTTP-Referer': 'https://github.com/weningerii/vtt',
          'X-Title': 'VTT Platform'
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`OpenRouter API error: ${response.status} ${response.statusText}`);
      }

      return response.json();
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class AnthropicChatProvider implements AIProvider {
  public readonly name = 'anthropic';
  public readonly type = 'chat' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    const requestBody = {
      model: options.model || 'claude-3-sonnet-20240229',
      max_tokens: options.maxTokens || 2000,
      temperature: options.temperature || 0.7,
      system: options.systemPrompt,
      messages: [
        ...(options.context || []).map(ctx => ({ role: 'user', content: ctx })),
        { role: 'user', content: prompt }
      ]
    };

    const response = await this.makeRequest('/messages', requestBody);
    
    return {
      text: response.content[0]?.text || '',
      usage: {
        promptTokens: response.usage?.input_tokens || 0,
        completionTokens: response.usage?.output_tokens || 0,
        totalTokens: (response.usage?.input_tokens || 0) + (response.usage?.output_tokens || 0)
      },
      model: response.model || requestBody.model,
      finishReason: response.stop_reason === 'end_turn' ? 'stop' : 'length'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}
    
Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated by Anthropic Claude'
    };
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.config.baseUrl || 'https://api.anthropic.com/v1'}${endpoint}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.config.apiKey}`,
          'anthropic-version': '2023-06-01'
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`Anthropic API error: ${response.status} ${response.statusText}`);
      }

      return response.json();
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class GeminiProvider implements AIProvider {
  public readonly name = 'gemini';
  public readonly type = 'chat' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation', 'vision'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    const requestBody = {
      contents: [
        {
          parts: [
            ...(options.systemPrompt ? [{ text: options.systemPrompt }] : []),
            { text: prompt }
          ]
        }
      ],
      generationConfig: {
        maxOutputTokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        topP: options.topP || 0.8
      }
    };

    const response = await this.makeRequest('/generateContent', requestBody, options.model);
    
    const text = response.candidates?.[0]?.content?.parts?.[0]?.text || '';
    const usage = response.usageMetadata || {};
    
    return {
      text,
      usage: {
        promptTokens: usage.promptTokenCount || 0,
        completionTokens: usage.candidatesTokenCount || 0,
        totalTokens: usage.totalTokenCount || 0
      },
      model: options.model || 'gemini-pro',
      finishReason: response.candidates?.[0]?.finishReason === 'STOP' ? 'stop' : 'length'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}
    
Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3,
      model: 'gemini-pro'
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated by Google Gemini'
    };
  }

  private async makeRequest(endpoint: string, body: any, model?: string): Promise<any> {
    const modelName = model || 'gemini-pro';
    const url = `${this.config.baseUrl || 'https://generativelanguage.googleapis.com/v1beta/models'}/${modelName}:${endpoint.replace('/', '')}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(`${url}?key=${this.config.apiKey}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
      }

      return response.json();
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class ReplicateProvider implements AIProvider {
  public readonly name = 'replicate';
  public readonly type = 'completion' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation', 'image_generation'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    // Default to Llama 2 70B Chat model
    const model = options.model || 'meta/llama-2-70b-chat';
    
    const requestBody = {
      version: 'latest',
      input: {
        prompt: options.systemPrompt ? `${options.systemPrompt}\n\nUser: ${prompt}\nAssistant:` : prompt,
        max_new_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        top_p: options.topP || 0.9,
        repetition_penalty: 1.15
      }
    };

    const response = await this.makeRequest(`/models/${model}/predictions`, requestBody);
    
    // Replicate returns an array of output strings
    const text = Array.isArray(response.output) ? response.output.join('') : response.output || '';
    
    return {
      text,
      usage: {
        promptTokens: this.estimateTokens(prompt),
        completionTokens: this.estimateTokens(text),
        totalTokens: this.estimateTokens(prompt) + this.estimateTokens(text)
      },
      model,
      finishReason: response.status === 'succeeded' ? 'stop' : 'length'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}

Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3,
      model: 'codellama/codellama-34b-instruct'
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated via Replicate open-source models'
    };
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.config.baseUrl || 'https://api.replicate.com/v1'}${endpoint}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Token ${this.config.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`Replicate API error: ${response.status} ${response.statusText}`);
      }

      const prediction = await response.json();
      
      // Poll for completion if prediction is still running
      if (prediction.status === 'starting' || prediction.status === 'processing') {
        return this.pollPrediction(prediction.id);
      }

      return prediction;
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private async pollPrediction(predictionId: string, maxAttempts: number = 30): Promise<any> {
    for (let attempt = 0; attempt < maxAttempts; attempt++) {
      await new Promise(resolve => setTimeout(resolve, 2000)); // Wait 2 seconds
      
      const response = await fetch(`${this.config.baseUrl || 'https://api.replicate.com/v1'}/predictions/${predictionId}`, {
        headers: {
          'Authorization': `Token ${this.config.apiKey}`
        }
      });

      if (!response.ok) {
        throw new Error(`Replicate polling error: ${response.status}`);
      }

      const prediction = await response.json();
      
      if (prediction.status === 'succeeded' || prediction.status === 'failed') {
        return prediction;
      }
    }
    
    throw new Error('Replicate prediction timed out');
  }

  private estimateTokens(text: string): number {
    // Rough estimation: ~4 characters per token
    return Math.ceil(text.length / 4);
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class HuggingFaceProvider implements AIProvider {
  public readonly name = 'huggingface';
  public readonly type = 'completion' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation', 'embeddings'];

  private config: AIProviderConfig;
  private rateLimitTracker = new Map<string, number[]>();

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    if (!this.checkRateLimit()) {
      throw new Error('Rate limit exceeded');
    }

    const model = options.model || 'microsoft/DialoGPT-large';
    
    const requestBody = {
      inputs: prompt,
      parameters: {
        max_new_tokens: options.maxTokens || 2000,
        temperature: options.temperature || 0.7,
        top_p: options.topP || 0.9,
        do_sample: true,
        return_full_text: false
      }
    };

    const response = await this.makeRequest(`/models/${model}`, requestBody);
    
    const text = Array.isArray(response) ? response[0]?.generated_text || '' : response.generated_text || '';
    
    return {
      text,
      usage: {
        promptTokens: this.estimateTokens(prompt),
        completionTokens: this.estimateTokens(text),
        totalTokens: this.estimateTokens(prompt) + this.estimateTokens(text)
      },
      model,
      finishReason: 'stop'
    };
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const codePrompt = `Generate ${options.language || 'TypeScript'} code for: ${prompt}

Requirements:
- Language: ${options.language || 'TypeScript'}
- Style: ${options.style || 'modern'}
- Complexity: ${options.complexity || 'intermediate'}
- Include comments and type definitions
- Return only the code without explanation`;

    const response = await this.generateText(codePrompt, {
      maxTokens: 4000,
      temperature: 0.3,
      model: 'codeparrot/codeparrot'
    });

    return {
      code: response.text,
      language: options.language || 'typescript',
      explanation: 'Code generated via HuggingFace Inference API'
    };
  }

  async createEmbeddings(text: string): Promise<number[]> {
    const response = await this.makeRequest('/models/sentence-transformers/all-MiniLM-L6-v2', {
      inputs: text
    });

    return Array.isArray(response) ? response : [];
  }

  private async makeRequest(endpoint: string, body: any): Promise<any> {
    const url = `${this.config.baseUrl || 'https://api-inference.huggingface.co'}${endpoint}`;
    
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.config.timeout);

    try {
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.config.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(body),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`HuggingFace API error: ${response.status} ${response.statusText}`);
      }

      return response.json();
    } finally {
      clearTimeout(timeoutId);
    }
  }

  private estimateTokens(text: string): number {
    // Rough estimation: ~4 characters per token
    return Math.ceil(text.length / 4);
  }

  private checkRateLimit(): boolean {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const key = 'requests';
    const requests = this.rateLimitTracker.get(key) || [];
    
    // Remove old requests outside the window
    const validRequests = requests.filter(time => time > windowStart);
    
    if (validRequests.length >= this.config.rateLimitPerMinute) {
      return false;
    }
    
    validRequests.push(now);
    this.rateLimitTracker.set(key, validRequests);
    
    return true;
  }
}

export class LocalAIProvider implements AIProvider {
  public readonly name = 'local';
  public readonly type = 'chat' as const;
  public readonly capabilities: AICapability[] = ['text_generation', 'code_generation'];

  private config: AIProviderConfig;

  constructor(config: AIProviderConfig) {
    this.config = config;
  }

  async generateText(prompt: string, _options: AITextOptions = {}): Promise<AITextResponse> {
    // Placeholder for local AI implementation
    const response = {
      text: `[Local AI Response] Based on: ${prompt.substring(0, 100)}...`,
      usage: {
        promptTokens: Math.floor(prompt.length / 4),
        completionTokens: 50,
        totalTokens: Math.floor(prompt.length / 4) + 50
      },
      model: 'local-model',
      finishReason: 'stop' as const
    };

    return response;
  }

  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    return {
      code: `// Generated code for: ${prompt}\n// Implementation placeholder for local AI`,
      language: options.language || 'typescript',
      explanation: 'Local AI code generation placeholder'
    };
  }
}

export class AIProviderRegistry extends EventEmitter<SystemEvents> implements Disposable {
  private providers = new Map<string, AIProvider>();
  private configs = new Map<string, AIProviderConfig>();
  private usageStats = new Map<string, AIUsageStats>();
  private requestHistory: AIRequest[] = [];
  private responseHistory: AIResponse[] = [];

  constructor() {
    super();
    this.initializeDefaultProviders();
  }

  /**
   * Register an AI provider
   */
  registerProvider(provider: AIProvider, config: AIProviderConfig): void {
    this.providers.set(provider.name, provider);
    this.configs.set(provider.name, config);
    
    // Reset stats on (re)registration to ensure clean slate per test/use
    this.usageStats.set(provider.name, {
      totalRequests: 0,
      successfulRequests: 0,
      failedRequests: 0,
      totalTokensUsed: 0,
      averageResponseTime: 0,
      costEstimate: 0
    });
  }

  /**
   * Get the best provider for a specific capability
   */
  getBestProvider(capability: AICapability): AIProvider | null {
    const availableProviders = Array.from(this.providers.entries())
      .filter(([name, provider]) => {
        const config = this.configs.get(name);
        return config?.enabled && provider.capabilities.includes(capability);
      })
      .sort(([nameA], [nameB]) => {
        const configA = this.configs.get(nameA);
        const configB = this.configs.get(nameB);
        if (!configA || !configB) {return 0;}
        return configB.priority - configA.priority;
      });

    return availableProviders.length > 0 ? availableProviders[0]![1] : null;
  }

  /**
   * Get all enabled providers supporting a capability, sorted by priority (desc)
   */
  getProvidersForCapability(capability: AICapability): Array<{ name: string; provider: AIProvider; config: AIProviderConfig }> {
    const candidates = Array.from(this.providers.entries())
      .map(([name, provider]) => ({ name, provider, config: this.configs.get(name) }))
      .filter((p): p is { name: string; provider: AIProvider; config: AIProviderConfig } => !!p.config && p.config.enabled && p.provider.capabilities.includes(capability))
      .sort((a, b) => b.config.priority - a.config.priority);

    return candidates;
  }

  /**
   * Generate text using the best available provider
   */
  async generateText(prompt: string, options: AITextOptions = {}): Promise<AITextResponse> {
    const candidates = this.getProvidersForCapability('text_generation');
    if (candidates.length === 0) {
      throw new Error('No text generation provider available');
    }

    return this.executeWithFailover(candidates, 'text', prompt, options);
  }

  /**
   * Generate code using the best available provider
   */
  async generateCode(prompt: string, options: AICodeOptions = {}): Promise<AICodeResponse> {
    const candidates = this.getProvidersForCapability('code_generation');
    if (!candidates.length) {
      throw new Error('No code generation provider available');
    }

    return this.executeWithFailover(candidates, 'code', prompt, options);
  }

  /**
   * Create embeddings using the best available provider
   */
  async createEmbeddings(text: string): Promise<number[]> {
    const provider = this.getBestProvider('embeddings');
    if (!provider) {
      throw new Error('No embeddings provider available');
    }

    if (!provider.createEmbeddings) {
      throw new Error('Provider does not support embeddings');
    }

    return provider.createEmbeddings(text);
  }

  /**
   * Smart content generation with context awareness for VTT
   */
  async generateVTTContent(type: 'character' | 'item' | 'spell' | 'encounter' | 'map_description', context: any): Promise<string> {
    const prompts = {
      character: this.buildCharacterPrompt(context),
      item: this.buildItemPrompt(context),
      spell: this.buildSpellPrompt(context),
      encounter: this.buildEncounterPrompt(context),
      map_description: this.buildMapPrompt(context)
    };

    const prompt = prompts[type];
    const response = await this.generateText(prompt, {
      maxTokens: 1000,
      temperature: 0.8
    });

    return response.text;
  }

  /**
   * Generate rule-based content
   */
  async generateRuleContent(ruleSystem: string, contentType: string, parameters: Record<string, any>): Promise<any> {
    const prompt = `Generate ${contentType} for ${ruleSystem} with the following parameters:
${JSON.stringify(parameters, null, 2)}

Please provide detailed, mechanically accurate content that follows the rules and conventions of ${ruleSystem}.`;

    const response = await this.generateText(prompt, {
      maxTokens: 1500,
      temperature: 0.6
    });

    try {
      // Try to parse as JSON first, fallback to text
      return JSON.parse(response.text);
    } catch {
      return response.text;
    }
  }

  /**
   * Get provider usage statistics
   */
  getUsageStats(providerId?: string): Map<string, AIUsageStats> {
    if (providerId) {
      const stats = this.usageStats.get(providerId);
      return stats ? new Map([[providerId, stats]]) : new Map();
    }
    return new Map(this.usageStats);
  }

  /**
   * Get available providers
   */
  getAvailableProviders(): Array<{ name: string; type: AIProvider['type']; capabilities: AICapability[]; enabled: boolean; priority: number }> {
    return Array.from(this.providers.entries()).map(([name, provider]) => ({
      name,
      type: provider.type,
      capabilities: provider.capabilities,
      enabled: this.configs.get(name)?.enabled || false,
      priority: this.configs.get(name)?.priority || 0
    }));
  }

  /**
   * Enable/disable a provider
   */
  toggleProvider(providerId: string, enabled: boolean): void {
    const config = this.configs.get(providerId);
    if (config) {
      config.enabled = enabled;
    }
  }

  /**
   * Dispose of the registry
   */
  dispose(): void {
    this.providers.clear();
    this.configs.clear();
    this.usageStats.clear();
    this.requestHistory = [];
    this.responseHistory = [];
    this.removeAllListeners();
  }

  // Private helper methods

  private async executeRequest(provider: AIProvider, type: string, prompt: string, options: any): Promise<any> {
    const requestId = this.generateRequestId();
    const request: AIRequest = {
      id: requestId,
      type: type as any,
      prompt,
      options,
      timestamp: new Date(),
      providerId: provider.name
    };

    this.requestHistory.push(request);
    
    const startTime = performance.now();
    let response: AIResponse;

    try {
      let result: any;
      
      switch (type) {
        case 'text':
          result = await provider.generateText!(prompt, options);
          break;
        case 'code':
          result = await provider.generateCode!(prompt, options);
          break;
        default:
          throw new Error(`Unsupported request type: ${type}`);
      }

      const processingTime = performance.now() - startTime;
      
      response = {
        requestId,
        success: true,
        data: result,
        usage: (result as any).usage,
        processingTime,
        providerId: provider.name
      };

      // Update stats
      this.updateUsageStats(provider.name, true, processingTime, (result as any).usage?.totalTokens || 0);

    } catch (error) {
      const processingTime = performance.now() - startTime;
      
      response = {
        requestId,
        success: false,
        error: error instanceof Error ? error.message : String(error),
        processingTime,
        providerId: provider.name
      };

      // Update stats
      this.updateUsageStats(provider.name, false, processingTime, 0);
      
      throw error;
    }

    this.responseHistory.push(response);
    return response.data;
  }

  /**
   * Execute a request with retries and provider failover
   */
  private async executeWithFailover(
    candidates: Array<{ name: string; provider: AIProvider; config: AIProviderConfig }>,
    type: 'text' | 'code',
    prompt: string,
    options: any
  ): Promise<any> {
    let lastError: unknown = null;

    for (const { name, provider, config } of candidates) {
      const attempts = Math.max(1, config.maxRetries || 1);

      for (let attempt = 1; attempt <= attempts; attempt++) {
        try {
          const timeoutMs = Math.max(1, config.timeout || 30000);
          // Reuse executeRequest for consistent stats and history tracking
          const result = await this.withTimeout(
            this.executeRequest(provider, type, prompt, options),
            timeoutMs
          );
          return result;
        } catch (err) {
          lastError = err;
          // If there are more attempts for this provider, continue retrying
          if (attempt < attempts) {
            continue;
          }
          // Otherwise, break to try next provider
          break;
        }
      }
    }

    throw new Error(
      `All providers failed${lastError instanceof Error ? `: ${lastError.message}` : ''}`
    );
  }

  private withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {
    return new Promise<T>((resolve, reject) => {
      const timer = setTimeout(() => reject(new Error('Request timed out')), ms);
      promise
        .then((val) => {
          clearTimeout(timer);
          resolve(val);
        })
        .catch((err) => {
          clearTimeout(timer);
          reject(err);
        });
    });
  }

  private updateUsageStats(providerId: string, success: boolean, processingTime: number, tokensUsed: number): void {
    const stats = this.usageStats.get(providerId);
    if (!stats) {return;}

    stats.totalRequests++;
    if (success) {
      stats.successfulRequests++;
    } else {
      stats.failedRequests++;
    }
    
    stats.totalTokensUsed += tokensUsed;
    stats.averageResponseTime = (stats.averageResponseTime * (stats.totalRequests - 1) + processingTime) / stats.totalRequests;
    stats.costEstimate += this.estimateCost(providerId, tokensUsed);
  }

  private estimateCost(providerId: string, tokensUsed: number): number {
    // Simplified cost estimation per 1K tokens
    const costs: Record<string, number> = {
      'openai': 0.002, // GPT-4 approximate
      'openrouter': 0.001, // Variable, using conservative estimate
      'anthropic': 0.003, // Claude Sonnet approximate
      'gemini': 0.0005, // Gemini Pro approximate
      'replicate': 0.0005, // Open source models, generally cheaper
      'huggingface': 0.0002, // Inference API pricing
      'local': 0
    };

    const costPer1K = costs[providerId] || 0.001; // Default fallback
    return (tokensUsed / 1000) * costPer1K;
  }

  private buildCharacterPrompt(context: any): string {
    return `Generate a detailed D&D 5e character description including:
- Name and basic appearance
- Race and class
- Background and personality traits
- Brief backstory
- Notable equipment

Context: ${JSON.stringify(context)}`;
  }

  private buildItemPrompt(context: any): string {
    return `Generate a magical item for D&D 5e including:
- Name and description
- Rarity and type
- Magical properties
- Lore and origin

Context: ${JSON.stringify(context)}`;
  }

  private buildSpellPrompt(context: any): string {
    return `Generate a custom D&D 5e spell including:
- Name and school of magic
- Level and casting requirements
- Duration and range
- Effects and flavor text

Context: ${JSON.stringify(context)}`;
  }

  private buildEncounterPrompt(context: any): string {
    return `Generate a D&D 5e encounter including:
- Setting and atmosphere
- Enemies or challenges
- Tactical considerations
- Potential rewards

Context: ${JSON.stringify(context)}`;
  }

  private buildMapPrompt(context: any): string {
    return `Generate a detailed map description including:
- Overall layout and geography
- Key locations and landmarks
- Atmosphere and mood
- Potential points of interest

Context: ${JSON.stringify(context)}`;
  }

  private initializeDefaultProviders(): void {
    const defaultConfig: AIProviderConfig = {
      apiKey: '',
      timeout: 30000,
      maxRetries: 3,
      rateLimitPerMinute: 60,
      priority: 1,
      enabled: false
    };

    // Initialize providers from environment if available
    const autoRegister = process.env.AI_ENABLE_AUTO_PROVIDERS !== 'false';
    const openaiKey = process.env.OPENAI_API_KEY;
    const openrouterKey = process.env.OPENROUTER_API_KEY;
    const anthropicKey = process.env.ANTHROPIC_API_KEY;
    const geminiKey = process.env.GOOGLE_API_KEY || process.env.GEMINI_API_KEY;
    const replicateKey = process.env.REPLICATE_API_TOKEN;
    const huggingfaceKey = process.env.HUGGINGFACE_API_KEY;

    // OpenAI Provider
    if (autoRegister && openaiKey && openaiKey.trim()) {
      this.registerProvider(
        new OpenAIProvider({ ...defaultConfig, apiKey: openaiKey, priority: 5, enabled: true }),
        { ...defaultConfig, apiKey: openaiKey, priority: 5, enabled: true }
      );
    }

    // OpenRouter Provider (highest priority for flexibility)
    if (autoRegister && openrouterKey && openrouterKey.trim()) {
      this.registerProvider(
        new OpenRouterProvider({ ...defaultConfig, apiKey: openrouterKey, priority: 4, enabled: true }),
        { ...defaultConfig, apiKey: openrouterKey, priority: 4, enabled: true }
      );
    }

    // Anthropic Provider
    if (autoRegister && anthropicKey && anthropicKey.trim()) {
      this.registerProvider(
        new AnthropicChatProvider({ ...defaultConfig, apiKey: anthropicKey, priority: 5, enabled: true }),
        { ...defaultConfig, apiKey: anthropicKey, priority: 5, enabled: true }
      );
    }

    // Gemini Provider
    if (autoRegister && geminiKey && geminiKey.trim()) {
      this.registerProvider(
        new GeminiProvider({ ...defaultConfig, apiKey: geminiKey, priority: 3, enabled: true }),
        { ...defaultConfig, apiKey: geminiKey, priority: 3, enabled: true }
      );
    }

    // Replicate Provider
    if (autoRegister && replicateKey && replicateKey.trim()) {
      this.registerProvider(
        new ReplicateProvider({ ...defaultConfig, apiKey: replicateKey, priority: 2, enabled: true, rateLimitPerMinute: 30 }),
        { ...defaultConfig, apiKey: replicateKey, priority: 2, enabled: true, rateLimitPerMinute: 30 }
      );
    }

    // HuggingFace Provider (inference API)
    if (autoRegister && huggingfaceKey && huggingfaceKey.trim()) {
      this.registerProvider(
        new HuggingFaceProvider({ ...defaultConfig, apiKey: huggingfaceKey, priority: 2, enabled: true, rateLimitPerMinute: 20 }),
        { ...defaultConfig, apiKey: huggingfaceKey, priority: 2, enabled: true, rateLimitPerMinute: 20 }
      );
    }

    // Optionally register local provider as fallback when explicitly enabled
    if (process.env.AI_ENABLE_LOCAL_PROVIDER === 'true') {
      this.registerProvider(
        new LocalAIProvider({ ...defaultConfig, priority: 1, enabled: true }),
        { ...defaultConfig, priority: 1, enabled: true }
      );
    }
  }

  private generateRequestId(): string {
    return `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }
}

// Export singleton instance
export const _aiProviderRegistry = new AIProviderRegistry();
