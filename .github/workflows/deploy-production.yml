# yaml-language-server: $schema=https://raw.githubusercontent.com/SchemaStore/schemastore/master/src/schemas/json/github-workflow.json
# checkov:skip=CKV_GHA_7: workflow_dispatch inputs are required for manual deployment control
name: Deploy to Production

permissions:
  contents: read

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        default: "production"
        type: choice
        options:
          - production
          - staging

env:
  NODE_VERSION: "20"
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # Security and Quality Checks
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
    steps:
      - uses: actions/checkout@v5

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: "fs"
          ignore-unfixed: true
          format: "sarif"
          output: "trivy-results.sarif"

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: "trivy-results.sarif"

  # Build and Test
  build-and-test:
    name: Build and Test
    runs-on: ubuntu-latest
    needs: [security-scan]
    strategy:
      matrix:
        app: [client, server]

    steps:
      - uses: actions/checkout@v5

      - name: Setup Node and pnpm
        uses: ./.github/actions/setup-node-pnpm
        with:
          node-version: ${{ env.NODE_VERSION }}
          pnpm-version: "9.12.2"
          install-system-packages: "nasm yasm build-essential"

      - name: Install dependencies
        uses: ./.github/actions/install-deps
        with:
          frozen-lockfile: "true"
          prefer-offline: "false"
          ensure-native-deps: "true"

      - name: Generate Prisma client
        run: pnpm db:gen

      - name: Lint
        run: pnpm turbo lint --filter @vtt/${{ matrix.app }}

      - name: Type check
        run: pnpm --filter @vtt/${{ matrix.app }} --if-present run typecheck

      - name: Run tests
        run: pnpm --filter @vtt/${{ matrix.app }} --if-present run test -- --coverage

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: apps/${{ matrix.app }}/coverage/lcov.info
          flags: ${{ matrix.app }}

  # Build Docker Images
  build-images:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: [build-and-test]
    permissions:
      contents: read
      packages: write

    strategy:
      matrix:
        app: [client, server]

    steps:
      - uses: actions/checkout@v5

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-${{ matrix.app }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}
            type=raw,value=${{ github.sha }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: apps/${{ matrix.app }}/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64,linux/arm64

  # End-to-End Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [build-images]

    steps:
      - uses: actions/checkout@v5

      - name: Setup Node and pnpm
        uses: ./.github/actions/setup-node-pnpm
        with:
          node-version: ${{ env.NODE_VERSION }}
          pnpm-version: "9.12.2"
          install-system-packages: "nasm yasm build-essential"

      - name: Install dependencies
        uses: ./.github/actions/install-deps
        with:
          frozen-lockfile: "true"
          prefer-offline: "false"

      - name: Start test environment
        run: docker-compose -f docker-compose.test.yml up -d
        env:
          CLIENT_IMAGE: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-client:${{ github.sha }}
          SERVER_IMAGE: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}-server:${{ github.sha }}

      - name: Wait for services
        run: |
          timeout 300 bash -c 'until curl -f http://localhost:3000/health; do sleep 5; done'
          timeout 300 bash -c 'until curl -f http://localhost:3001/api/v1/health; do sleep 5; done'

      - name: Run E2E tests
        run: pnpm run test:e2e
        env:
          BASE_URL: http://localhost:3000
          API_URL: http://localhost:3001

      - name: Upload E2E test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: e2e-results
          path: e2e/test-results/

  # Deploy to Staging
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [e2e-tests]
    if: github.ref == 'refs/heads/main' || github.event.inputs.environment == 'staging'
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      TERRAFORM_STATE_BUCKET: ${{ secrets.TERRAFORM_STATE_BUCKET }}
    # environment: staging  # Uncomment when staging environment is configured in repo settings

    steps:
      - uses: actions/checkout@v5

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Configure AWS credentials
        if: ${{ env.AWS_ACCESS_KEY_ID != '' && env.AWS_SECRET_ACCESS_KEY != '' }}
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Deploy infrastructure
        if: ${{ env.TERRAFORM_STATE_BUCKET != '' && env.AWS_ACCESS_KEY_ID != '' }}
        working-directory: ./infra/terraform
        run: |
          terraform init -backend-config="bucket=${{ secrets.TERRAFORM_STATE_BUCKET }}"
          terraform workspace select staging || terraform workspace new staging
          terraform plan -var-file="staging.tfvars" -out=tfplan
          terraform apply -auto-approve tfplan

      - name: Deploy to Kubernetes
        run: |
          aws eks update-kubeconfig --region us-east-1 --name vtt-staging

          # Update image tags in k8s manifests
          sed -i "s|IMAGE_TAG|${{ github.sha }}|g" infra/k8s/staging/*.yaml

          # Apply manifests
          kubectl apply -f infra/k8s/staging/

          # Wait for rollout
          kubectl rollout status deployment/vtt-client -n vtt-staging
          kubectl rollout status deployment/vtt-server -n vtt-staging

      - name: Run smoke tests
        run: |
          STAGING_CLIENT=$(kubectl get ingress vtt-staging -n vtt-staging -o jsonpath='{.spec.rules[0].host}')
          STAGING_API=$(kubectl get ingress vtt-staging -n vtt-staging -o jsonpath='{.spec.rules[1].host}')
          curl -f https://$STAGING_CLIENT/health
          curl -f https://$STAGING_API/api/v1/health

  # Deploy to Production
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [deploy-staging]
    if: github.ref == 'refs/heads/main' && github.event.inputs.environment != 'staging'
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      TERRAFORM_STATE_BUCKET: ${{ secrets.TERRAFORM_STATE_BUCKET }}
    # environment: production  # Uncomment when production environment is configured in repo settings

    steps:
      - uses: actions/checkout@v5

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Configure AWS credentials
        if: ${{ env.AWS_ACCESS_KEY_ID != '' && env.AWS_SECRET_ACCESS_KEY != '' }}
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Deploy infrastructure
        if: ${{ env.TERRAFORM_STATE_BUCKET != '' && env.AWS_ACCESS_KEY_ID != '' }}
        working-directory: ./infra/terraform
        run: |
          terraform init -backend-config="bucket=${{ secrets.TERRAFORM_STATE_BUCKET }}"
          terraform workspace select production || terraform workspace new production
          terraform plan -var-file="production.tfvars" -out=tfplan
          terraform apply -auto-approve tfplan

      - name: Deploy to Kubernetes (Blue-Green)
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}
          ACM_CERTIFICATE_ID: ${{ secrets.ACM_CERTIFICATE_ID }}
          WAF_WEB_ACL_ID: ${{ secrets.WAF_WEB_ACL_ID }}
          PROD_CLIENT_DOMAIN: ${{ vars.PROD_CLIENT_DOMAIN }}
          PROD_API_DOMAIN: ${{ vars.PROD_API_DOMAIN }}
        run: |
          aws eks update-kubeconfig --region us-east-1 --name vtt-production

          # Update image tags in k8s manifests
          sed -i "s|IMAGE_TAG|${{ github.sha }}|g" infra/k8s/production/*.yaml

          # Replace AWS account and certificate placeholders if provided
          if [ -n "${AWS_ACCOUNT_ID}" ]; then
            sed -i "s|ACCOUNT_ID|${AWS_ACCOUNT_ID}|g" infra/k8s/production/ingress.yaml
            sed -i "s|ACCOUNT_ID|${AWS_ACCOUNT_ID}|g" infra/k8s/production/server-deployment.yaml
          fi
          if [ -n "${ACM_CERTIFICATE_ID}" ]; then
            sed -i "s|CERT_ID|${ACM_CERTIFICATE_ID}|g" infra/k8s/production/ingress.yaml
          fi
          if [ -n "${WAF_WEB_ACL_ID}" ]; then
            sed -i "s|WAF_ID|${WAF_WEB_ACL_ID}|g" infra/k8s/production/ingress.yaml
          fi

          # Replace domains if repository variables are set
          if [ -n "${PROD_CLIENT_DOMAIN}" ]; then
            sed -i "s|vtt.platform.com|${PROD_CLIENT_DOMAIN}|g" infra/k8s/production/ingress.yaml
            sed -i "s|https://vtt.platform.com|https://${PROD_CLIENT_DOMAIN}|g" infra/k8s/production/configmap.yaml
          fi
          if [ -n "${PROD_API_DOMAIN}" ]; then
            sed -i "s|api.vtt.platform.com|${PROD_API_DOMAIN}|g" infra/k8s/production/ingress.yaml
            sed -i "s|https://api.vtt.platform.com|https://${PROD_API_DOMAIN}|g" infra/k8s/production/configmap.yaml
            sed -i "s|wss://api.vtt.platform.com|wss://${PROD_API_DOMAIN}|g" infra/k8s/production/configmap.yaml
          fi

          # Apply all manifests (both colors)
          kubectl apply -f infra/k8s/production/

          # Wait for green deployments to be ready
          kubectl rollout status deployment/vtt-client-green -n vtt-production
          kubectl rollout status deployment/vtt-server-green -n vtt-production

          # Run health checks directly against a green server pod
          GREEN_POD=$(kubectl get pod -n vtt-production -l app.kubernetes.io/name=vtt-platform,app.kubernetes.io/component=server,version=green -o jsonpath='{.items[0].metadata.name}')
          kubectl port-forward pod/$GREEN_POD -n vtt-production 3001:3001 &
          PF_PID=$!
          sleep 5
          curl -f http://localhost:3001/api/v1/health
          kill $PF_PID || true

          # Switch traffic to green by patching Service selectors (client, server, websocket)
          kubectl patch service vtt-client -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"client","version":"green"}}}'
          kubectl patch service vtt-server -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"server","version":"green"}}}'
          kubectl patch service vtt-websocket -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"server","version":"green"}}}'

          # Verify production endpoints
          sleep 20
          PROD_CLIENT=$(kubectl get ingress vtt-production -n vtt-production -o jsonpath='{.spec.rules[0].host}')
          PROD_API=$(kubectl get ingress vtt-production -n vtt-production -o jsonpath='{.spec.rules[1].host}')
          curl -f https://$PROD_CLIENT/health
          curl -f https://$PROD_API/api/v1/health

          # Scale down blue and suspend blue HPAs (minReplicas=0) to avoid scale-ups
          kubectl scale deployment/vtt-client-blue -n vtt-production --replicas=0
          kubectl scale deployment/vtt-server-blue -n vtt-production --replicas=0
          kubectl patch hpa vtt-client-hpa -n vtt-production --type merge -p '{"spec":{"minReplicas":0}}'
          kubectl patch hpa vtt-server-hpa -n vtt-production --type merge -p '{"spec":{"minReplicas":0}}'

  # Post-deployment
  post-deploy:
    name: Post-deployment Tasks
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: always() && needs.deploy-production.result == 'success'
    env:
      SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}

    steps:
      - name: Notify Slack
        if: ${{ env.SLACK_WEBHOOK != '' }}
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        uses: 8398a7/action-slack@v3
        with:
          status: success
          channel: "#deployments"

      # Removed 'Update deployment status' step due to non-deployment events in this workflow

  # Rollback capability
  rollback:
    name: Rollback Deployment
    runs-on: ubuntu-latest
    if: failure() && github.ref == 'refs/heads/main'
    env:
      SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}

    steps:
      - uses: actions/checkout@v5

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Rollback production deployment
        run: |
          aws eks update-kubeconfig --region us-east-1 --name vtt-production

          # Switch service selectors back to blue
          kubectl patch service vtt-client -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"client","version":"blue"}}}'
          kubectl patch service vtt-server -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"server","version":"blue"}}}'
          kubectl patch service vtt-websocket -n vtt-production -p '{"spec":{"selector":{"app.kubernetes.io/name":"vtt-platform","app.kubernetes.io/component":"server","version":"blue"}}}'

          # Restore blue capacity and reduce green
          kubectl patch hpa vtt-client-hpa -n vtt-production --type merge -p '{"spec":{"minReplicas":3}}'
          kubectl patch hpa vtt-server-hpa -n vtt-production --type merge -p '{"spec":{"minReplicas":3}}'
          kubectl scale deployment/vtt-client-blue -n vtt-production --replicas=3
          kubectl scale deployment/vtt-server-blue -n vtt-production --replicas=3
          kubectl scale deployment/vtt-client-green -n vtt-production --replicas=0
          kubectl scale deployment/vtt-server-green -n vtt-production --replicas=0

          # Wait for rollback to complete
          kubectl rollout status deployment/vtt-client-blue -n vtt-production
          kubectl rollout status deployment/vtt-server-blue -n vtt-production

          echo "🔄 Rollback completed to blue version"

      - name: Notify rollback
        if: ${{ env.SLACK_WEBHOOK != '' }}
        env:
          SLACK_WEBHOOK: ${{ secrets.SLACK_WEBHOOK }}
        uses: 8398a7/action-slack@v3
        with:
          status: "warning"
          channel: "#deployments"
